\chapter{Controllable Fusion using Knowledge Graph Paths}
\label{chap:controllable_fusion}

% Comment: This chapter focuses on the methodology for controllable fusion using KG paths

While constraining the answer type, as discussed in Chapter~\ref{chap:candidate_generation}, provides a valuable signal for improving factuality, another powerful approach involves leveraging the explicit relational structure within the Knowledge Graph more directly. This chapter introduces methods based on the core idea of using KG subgraphs - a small part of the KG that contains the entities from the question, answer candidate generated by the LLM, and the shortest paths between them, drawing upon the work presented in \cite{DBLP:journals/corr/abs-2310-02166} and subsequent refinements focused on reranking 
{\color{red} TODO: add cite SWJ Reranking Answers of Large Language Models with Knowledge Graphs}.

The central motivation for this chapter is to move beyond the knowledge concentrated in parameters of the LLM and instead use pathes derived from the Knowledge Graph. This is similar to how people search in knowledge graphs: they go step-by-step, or hop-by-hop, through connected entities to find answer and become sure that entities are really related. Instead of only using LLM's not transparent generation process for factuality, we suggest using the paths that connect entities from question to potential answer entities in the KG. Such paths are good indicator of factual correctness, giving more control and interpretability, because KG path itself gives concrete evidence that supports given answer.

The general pipeline for this fusion strategy, illustrated in Figure~\ref{fig:controllable_fusion:big_pipe}, involves several stages. First, relevant entities are identified within the input question. Second, candidate answers might be generated by an LLM, or potential answer entities are retrieved from the KG, focusing on entities connected to the question entities via relatively short paths. Crucially, features are extracted from these connecting paths (or the surrounding subgraph). These features capture information about the path structure and relations involved. Finally, these subgraph based features are used by a scoring or reranking model to evaluate the correctness of each candidate answer, ultimately selecting the answer best supported by the explicit structure of the Knowledge Graph. This chapter will detail the specific mechanisms developed for implementing this subgraph based fusion method.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth]{kg_path_fusion/new_paper_big_pipeline.pdf}
    \caption{The proposed method for reranking language model answers with KGs. The method includes subgraph extraction, features extraction, and various ranker approaches.}
    \label{fig:controllable_fusion:big_pipe}
\end{figure}

\section{Answer Candidate Generation}
\label{sec:controllable_fusion:answer_candidate_generation}

As the subgraph extraction protocol requires answer candidates, we need a source of distinct answer candidates for each question, but most LLM approaches for QA, such as the one presented by \cite{DBLP:conf/coling/SenAS22-mintaka}, typically use \texttt{Greed Search} and evaluate the top-1 answer, it is important to note that the correct answer may not always be the top candidate. For example, the fine-tuned T5-XL-SSM~\cite{DBLP:conf/emnlp/RobertsRS20} model achieved higher Mean Reciprocal Rank (MRR) scores for our task, indicating that re-ranking could improve the top-1 results. 

Similar to the previous discussed method in Section~\ref{sec:candidate_generation:initial_answer_candidate_generation} to generate initial answer candidates, we use Diverse Beam Search~\cite{DBLP:journals/corr/VijayakumarCSSL16-diverse-beam-search} to generate an initial list of answer candidates. This candidates will be used for subgraph extraction and features extraction to rank them and get the final answer for the questions.
  
% In this section, we examine each component of the process in detail.
%  Firstly, we generate answer candidates using various LLMs and generate subgraphs, as discussed in Subsection~\ref{sec:subgraph_extract}. Next, in Subsection~\ref{sec:subgraph_features}, we will look at which attributes are used to rank responses.


% We apply {Diverse Beam Search} to the following LLMs: {T5-large-ssm}, {T5-XL-ssm}, {Mistral}, and {Mixtral} with $200$ beams, $20$ beam groups, and a $0.1$ diversity penalty. We extend our previous research~\cite{DBLP:conf/paclic/SalnikovLRNBMP23-originalpaper} by fine-tuning the proposed T5-like models and comparing them to more recent state-of-the-art models like Mistral and Mixtral, which should make our research more applicable to real-world use cases. T5-large-SSM and T5-XL-SSM were reported to be state-of-the-art both in the original Mintaka paper and our previous work, serving as a good baseline for comparison in this study.

% To finetune the T5-like models, we first train them on English questions for $10000$ steps, following the protocols outlined in the original Mintaka paper~\cite{DBLP:conf/coling/SenAS22-mintaka}. For the more state-of-the-art Mistral and Mixtral, we finetune with LoRA and train on English questions by generating the answer candidates with ``\textit{Answer as briefly as possible without additional information. [Question]}''. However, for the T5-like models, despite adhering to these protocols, we could not achieve the reported Hits@1 accuracy in the original paper. Despite this challenge, the main focus of the study is on the reranking aspect of the pipeline. Therefore, this paper's primary contribution is improving our fine-tuned models.

\section{Subgraph Extraction}
\label{sec:controllable_fusion:subgraph_extract}
The main backbone of our approach is the procedure of subgraph extraction. We rely on the information conveyed in the relationships between question-answer pairs to improve the reranking of LLM generations. To further investigate how this relationship can improve performance, we employ a subgraph extraction algorithm that generates a KG's subgraph containing entities relevant to each question-answer pair and the shortest paths between them that contain relevant properties/relationships. 
% We extract various features that can be used for reranking in addition to the subgraphs. This section presents the subgraph extraction algorithm, the features derived from the subgraphs, and our reranking approaches.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth]{kg_path_fusion/ssp_to_sub.pdf}
    \caption{The proposed method for reranking language model answers with KGs. The method includes subgraph extraction, features extraction, and various ranker approaches.}
    \label{fig:controllable_fusion:subgraph_construction_example}
\end{figure}

For each question-answer candidate pair, the desired subgraph $G$ is mathematically defined as an induced subgraph of the Knowledge Graph. Thus, given our shortest paths from $e_i~\rightarrow~A$, where $e_i$ - entity extracted from question and $A$ - Answer. We can use the following Listing~\ref{alg:controllable_fusion:sub_extract} to extract $G$. Let us define $H$ as the set of all distinct nodes within our shortest paths $P_i$. We want to preserve all edges between the nodes within $H$. For all question-answer pairs, our objective is to retain the relationship between our question entities $E$ and answer candidate entity $A_i$. The process is schematically depicted at Figure~\ref{fig:controllable_fusion:subgraph_construction_example}.

\begin{ListingEnv}[p]
    \centering % Center the listing if desired
    \caption{Subgraph Extraction Algorithm} 
    \label{alg:controllable_fusion:sub_extract} 
    \begin{lstlisting}[basicstyle=\fontsize{10pt}{12pt}\selectfont\ttfamily] % Smaller font for code Require: entities, candidate

paths = []
For entity in entities:
    shortest_paths = get_shortest_path(entity, candidate)
    paths.extend(shortest_paths)

H = set of unique nodes in paths

G = new Graph()
Add nodes from H to G

For unique_node in H:
    unique_node_neighbors = get_neighbors(unique_node)
    For neighbor_node in unique_node_neighbors:
        If neighbor_node in H:
            Add edge (unique_node, neighbor_node) to G

Return G
    \end{lstlisting}
\end{ListingEnv}


\section{Features based on Extracted Subgraphs}
\label{sec:controllable_fusion:subgraph_features}
After extracting subgraphs for all answer candidates of our LMs, we use all possible useful features for reranking. Referring to our previous study, we mainly focused on a simple text representation of the extracted subgraphs to rank our answer candidates. Thus, in this study, we propose extracting as many useful features as possible and analyzing each feature's importance in this reranking problem. We have divided the features into the following main categories: graph, text, and Graph2Text sequence features. 

\subsection{Graph Features}
\label{sec:controllable_fusion:graph_features}
With our extracted subgraphs and their corresponding answer candidate, we seek to use the relationship from the subgraphs to classify the correct answer candidate. As the first simple baseline, we utilize graph features consisting of simple numerical subgraph statistics. We hypothesize that subgraphs with the correct answer will be less ``complex'' than subgraphs with the incorrect answer candidate. Therefore, we would want the graph features to convey the complexity of the respective subgraph. With a clear objective in mind, we experiment with the following graph features:  

\begin{itemize}
    \item \textbf{Number of nodes and edges}: basic statistics of the nodes and edges of graph $G$.
    \item \textbf{Number of cycles}: a cycle of graph $G$ is a non-empty path that starts from a given node and ends at the same node. 
    \item \textbf{Number of bridges}: a bridge of graph $G$ is an edge, where its deletion increases the number of connection components. 
    \item \textbf{Average shortest path}: the average of each shortest path between the question entity and the answer entity. 
    \item \textbf{Density}: measurement of the density of a graph, where the number of edges in a dense graph is close to the maximal number of edges (each pair of nodes is connected by an edge). The density $d$ for the graph $G$ is formulated as $d = \frac{m}{n(n-1)}$, where $n$ is the number of nodes and $m$ is the number of edges in $G$.
   \item \textbf{Katz centrality}~\cite{katz1953new}: measurement of the importance (or ``centrality'' - how ``central'' a node is in the graph) of a specific node $i$ in a graph $G$. The Katz centrality for node $i$ of graph $G$ is formulated as $x_i = \alpha \sum_{j} A_{ij} x_j + \beta$, where $A$ is the adjacency matrix of graph $G$ with eigenvalues $\lambda$, $\beta$ is the parameter that controls the initial centrality, and $\alpha < \frac{1}{\lambda_{\max}}$. 
    \item \textbf{PageRank}~\cite{page1999pagerank}: a popular algorithm used by Google to rank web pages in the search query by counting the number and quality of links to a page to determine an estimate of its importance. In graph theory, the ``web pages'' and ``links'' are synonymous with nodes and edges. 
\end{itemize}

\noindent We hypothesize that these features may provide ranker models with insights into the complexity of the respective subgraphs.


\subsection{Text Features}
\label{sec:controllable_fusion:text_features} 
The ablation study showcased the importance of including the question within the text representation of the subgraph. Therefore, besides the simple graph features, we want to emphasize each question/answer pair without using extracted subgraphs. Thus, the text features represent the concatenation between the question and answer, separated by a semicolon --- ``\texttt{;}''. To use this simple concatenation for all ranker approaches, we encode the string using the MPNet\footnote{\url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}} embedding model~\cite{DBLP:conf/nips/Song0QLL20}.


\subsection{Graph2Text Sequence Features}
\label{sec:controllable_fusion:g2t_seq} 
Given the vast amount of data contained in Knowledge Graphs, it is essential to convert this information into natural language to facilitate understanding and accessibility. Converting a graph into text, known as KG-to-text or Graph2Text, has demonstrated notable success in various applications~\cite{DBLP:journals/corr/abs-2309-11206}. Therefore, when generating text from a Knowledge Graph, it is crucial to analyze the underlying graph structure carefully to ensure accurate translation.

Without an obvious way of incorporating the question within the subgraphs, relying purely on the subgraphs to rerank is ineffective~\cite{DBLP:conf/paclic/SalnikovLRNBMP23-originalpaper}. Therefore, we address this issue by further exploration of different KG-to-text methods. The main objective is experimenting with various techniques to represent the extracted subgraphs more explicitly. For this type of textual feature, we researched and developed three methods for representing subgraphs as a text, including \textbf{Graph2Text Deterministic,  Graph2Text T5, and  Graph2Text GAP}.

Firstly, we employ the \textbf{Graph2Text Deterministic} approach, the most straightforward text linearization approach. In simple terms, the subgraphs are unraveled by their matrix representation. Firstly, to linearize, we convert the subgraph into its binary adjacency matrix. Let us call it matrix $A$. 
Given $n$ nodes in the subgraph, the resulting matrix's dimension will be $n \times n$. The matrix's element $[i, j]$ represents the existence of an edge between a node with index $i$ and a node with index $j$. Then, we replace the edges in the matrix with the edge label and call it $A'$. 
Adjacency matrices are typically implemented with graphs with numeric weights. The weights were string representing the relationship between our nodes. Thus, we represented the existence of an edge with $1$, then replaced in the edge with the string relationship.
%Let us call this adjacency matrix with edge information . 
Lastly, we unravel $A'$ row by row to produce our final sequence and add the triple (node\_from, edge, node\_to) to our final sequence. Listing~\ref{alg:controllable_fusion:sub2seq} summarizes the aforementioned steps. 

\begin{ListingEnv}[p]
    \centering % Center the listing if desired
    \caption{Subgraphs to Sequence - Graph2Text Deterministic} 
    \label{alg:controllable_fusion:sub2seq} 
    \begin{lstlisting}[basicstyle=\fontsize{10pt}{12pt}\selectfont\ttfamily] % Smaller font for code

Require: Subgraph G
Ensure: Text representation of subgraph Seq

adj_matrix = get_adjacency_matrix(G)
Seq = ""
# Assuming adj_matrix is n x n, where n is number of nodes
# and indices correspond to node IDs
For i in range(number_of_nodes(G)): 
    For j in range(number_of_nodes(G)):
        # Assuming 0 indicates no edge, non-zero indicates an edge
        If adj_matrix[i][j] != 0: 
            # Assuming G allows lookup by index i, j
            node_i_label = get_node_label(G, i) 
            node_j_label = get_node_label(G, j)
            # Get edge info (label/type) between node i and j
            edge_info = get_edge_info(G, i, j) 
            # Append triple to sequence string, separated by e.g., semicolon
            Seq += node_i_label + " " + edge_info + " " + node_j_label + "; " 
Return Seq
    \end{lstlisting}
\end{ListingEnv}

For the remaining two text linearization approaches, \textbf{Graph2Text T5} and \textbf{Graph2Text GAP}, we employ more complex neural-based models trained on the WebNLG~2.0 dataset~\cite{DBLP:conf/acl/GardentSNP17}. This dataset consists of instances, where each includes a Knowledge Graph from DBpedia~\cite{DBLP:conf/semweb/AuerBKLCI07} and a target text comprising one or more sentences that describe the graph. The test set is divided into partitions of seen (DBpedia categories present in the training set) and unseen (DBpedia categories not present in the training set). The statistics of this hand-crafted and human-verified dataset are described in detail in Table~\ref{tab:controllable_fusion:webnlg_label}. 

\begin{table}
    \centering
    \caption{Statistics of the WebNLG 2.0 parallel knowledge graph-to-text dataset.}
    
    \begin{subtable}[t]{0.48\textwidth}
    \centering
    \begin{tabular}{lr}
        \toprule
        \textbf{Entities} & 2,730 \\
        \textbf{Relations} & 354 \\
        \textbf{Triples} & 81,927 \\
        \bottomrule
    \end{tabular}
    \caption{Knowledge Graph statistics. Total number of KG components, number of tokens in the narratives.}
    \label{tab:kg_stats}
    \end{subtable}
    \hfill
    \begin{subtable}[t]{0.48\textwidth}
    \centering
    \begin{tabular}{lr}
        \toprule
        \textbf{Total} & 623,902 \\
        \textbf{Unique} & 8,075 \\
        \textbf{Entity} & 60\% \\
        \bottomrule
    \end{tabular}
    \caption{Texts statistics. The percentage of text entities represents the portion of the text that includes entity labels.}
    \label{tab:controllable_fusion:webnlg_label}
    \end{subtable}
\end{table}

The idea behind the \textbf{Graph2Text T5} approach is to extract informative and useful features from KGs using pre-trained text-to-text LMs. With the impressive capabilities of pretrained LMs in the text-to-text generation task, we seek to replicate such results in the graph-to-text scope. Our idea is built upon the analogous algorithm discussed in~\cite{DBLP:journals/corr/abs-2007-08426}. The authors tackle the graph-to-text generation task in this work with two popular text-to-text pre-trained LMs, BART and T5. These models have an encoder-decoder architecture, which makes them well-suited for conditional text generation tasks. To adapt these models for the graph-to-text task, the authors continue pre-training BART and T5 using the following approaches:

\begin{enumerate}
    \item Language Model Adaptation (LMA): the models are trained on reference texts that describe graphs, following the BART and T5 pre-training strategies.
    \item Supervised Task Adaptation (STA): the models are trained on pairs of graphs and their corresponding texts collected from the same or a similar domain as the target task --- graph-to-text in this case. 
\end{enumerate}

Building on the STA approach via T5 and WebNLG~2.0, we obtain graph-to-text sequences by first converting the graph into a sequence of tokens through linearization. We use the string ``convert the [graph] to [text]:'' to acquire this linearised sequence. This output sequence is then fed into the input sequence for the T5 model tuned on WebNLG~2.0. 
% For tuning Graph2Text T5 approach, we use the following hyperparameters: \textit{learning rate}: $1e^{-3}$, \textit{batch size}: 4, \textit{gradient accumulation steps}: 32, and \textit{Adam optimizer}. 

The \textbf{Graph2Text GAP} approach is based on the current state-of-the-art graph-to-text task, GAP, built on BART~\cite{DBLP:conf/coling/ColasAW22-GAP}. 
The main idea of GAP is a fully graph-aware encoding combined with the coverage of pre-trained LMs. The GAP KG-to-text framework fuses graph-aware elements into existing pre-trained LMs, capturing the advantages brought forth by both model types. The architecture of this solution consists of two main components:

\begin{enumerate}
\item \textbf{Global Attention}: to capture the graph's global semantic information, the graph's components are first encoded using an LM. This allows the model to leverage the lexical coverage of pre-trained LMs.
\item \textbf{Graph-aware Attention}: to attend to and update the representations of entities, relations, or both, a topological-aware graph attention mechanism was introduced, which includes entity and relation type encoding.
\end{enumerate}

Applying the work of GAP, we first linearize the input graph into a text string by creating a sequence of all triples in the KG, interleaved with tokens that separate each triple and the triple's components (head, relation, and tail). Then, we use a transformer encoder to obtain vector representations. The first module in each transformer layer acts as a Global Attention and captures the semantic relationships between all tokens. Moreover, we use a Graph-aware Attention module to capture the sparse nature of adjustment in a graph and apply it to entity and relation vectors from word vectors. By proposing this flexible framework, where graph-aware components can be interchanged, the current architecture aims to generate coherent and representative text descriptions of the KG. Like the Graph2Text T5 approach, we pretrain the model on the WebNLG~2.0 dataset and get the final predictions through the fine-tuned model. 
% For finetuning the Graph2text GAP approach, we use the following hyperparameters: \textit{learning rate}: $2e^{-5}$, \textit{batch size}: 16; \textit{beam size}: 5, \textit{Adam Optimizer}, 50 \textit{nodes}, and 60 \textit{relations}.

\section{Rank LLM answer candidates using subgraphs} \label{sec:controllable_fusion:ranker}
With the subgraphs and their extracted features discussed above, we devise several reranking approaches to maximize the performance of the base large language models. As the focal point of the research is the reranking scope, we employ reranking methodologies from least to most complex. The hypothesis is a positive trend in performance as we apply more complex models and features.

As a starting point, we employ semantic reranking. This is a popular solution in information retrieval~\cite{DBLP:journals/jifs/FigueroaPP20, DBLP:journals/corr/HendersonASSLGK17}, implemented differently under the same name. Building on this foundation, our semantic ranker utilizes the MPNET~\cite{DBLP:conf/nips/Song0QLL20} embeddings of the answer candidates, further justified in~\ref{mpnet_explain}. We then rank the answer candidates by the cosine similarity between the embedding vectors. 

In the next layer of complexity, we utilize regression-based models, namely, linear and logistic regression. %~\cite{legendre1806nouvelles}
For the features set, we apply all features discussed in~\ref{sec:controllable_fusion:subgraph_features} (for features in text/string format, we apply MPNET embeddings, discussed in~\ref{mpnet_explain}). In the case of linear regression, we employ ordinary least squares linear regression to predict either $1$ or $0$, corresponding to correct and incorrect responses, respectively. The predicted score was then used to rank the potential answers by sorting the values from highest to lowest. Although we employ logistic regression for the same reranking task, we reformat the problem to a classic classification problem. We sort the answers with the highest classification confidence to rank the candidates. We use a standard logistic regression model with L2 regularisation. 

In addition to regression-based models, we want to utilize the same features with a more complex ranker. Thus, we explore and experiment with the gradient boosting model, specifically the CatBoost regression model~\cite{DBLP:conf/nips/ProkhorenkovaGV18-catboost}. Before training, we use grid search to finetune \textit{learning\_rate, depth,} and \textit{iteration}. We use root-mean-square deviation (RMSE) to evaluate. Like linear regression, we use the predicted score to rank the answer candidates by sorting the values. 

Last but not least, the most effective and complex approach for reranking answer candidates is a neural-based ranker with textual features as input. We experiment with a transformer-based model with an additional regression head layer, which is fine-tuned using {mean-square loss} and {AdamW} optimisation~\cite{DBLP:conf/iclr/LoshchilovH19-adamw}. To keep the experiments clear and transparent, we keep the same MPNet model for this ranker. We employ this variation of sentence transformer throughout this research for various sentence/text embeddings, as mentioned in~\ref{mpnet_explain}. Due to the lack of a straightforward method for utilizing numeric or table-like features with this ranker, we choose not to apply graph features. 


\section{Experiments}
In this section, we describe an experimental setup to test the usability of our proposed language model reranking methods based on KGs. We explore the impact of different combinations of features on reranking performance and examine the effects of various reranking methods, ranging from simple to more sophisticated, on reranking accuracy.

\subsection{Dataset} \label{sec:dataset}
To further enhance the results gathered in the original paper, we also conduct our research on Mintaka~\cite{DBLP:conf/coling/SenAS22-mintaka} dataset, which is a large-scale, complex and natural dataset that can be used for end-to-end question-answering models, composed of $20,000$ question-answer pairs. This dataset is annotated with Wikidata entities and comprises 8 types of complex questions. These types include: 
\begin{itemize}%[noitemsep]
    \item \textbf{Count}, e.g., Q: How many astronauts have been elected to Congress? A: 4.
    \item \textbf{Comparative}, e.g., Q: Is Mont Blanc taller than Mount Rainier? A: Yes.
    \item \textbf{Superlative}, e.g., Q: Who was the youngest tribute in the Hunger Games? A: Rue.
    \item \textbf{Ordinal}, e.g., Q: Who was the last Ptolemaic ruler of Egypt? A: Cleopatra.
    \item \textbf{Multi-hop}, e.g., Q: Who was the quarterback of the team that won Super Bowl 50? A: Peyton Manning.
    \item \textbf{Intersection}, e.g., Q: Which movie was directed by Denis Villeneuve and stars Timothee Chalamet? A: Dune.
    \item \textbf{Difference}, e.g., Q: Which Mario Kart game did Yoshi not appear in? A: Mario Kart Live: Home Circuit.
    \item \textbf{Yes/No}, e.g., Q: Has Lady Gaga ever made a song with Ariana Grande? A: Yes.
    \item \textbf{Generic}, e.g., Q: Where was Michael Phelps born? A: Baltimore, Maryland.
\end{itemize}

Factoid questions can sometimes have answers that cannot be linked to an entity in knowledge bases (count and yes/no questions). To handle this edge case in a production system, one could first train a classifier to categorize questions into three types: yes/no, count, and other. For this classification task, we train the MPNet (all-mpnet-base-v2)~\cite{DBLP:conf/nips/Song0QLL20} model using CrossEntropy loss. The training is performed over $5$ epochs on the Mintaka train split, utilizing a batch size of $32$, $500$ warm-up steps, a weight decay of $0.01$, and a learning rate of $0.00005$. To manage the training data, we employ the HuggingFace basic Trainer along with a Weighted Random Sampler. This approach achieves a balanced accuracy of \textbf{98.29\%} on the Mintaka test split.

This research centers around the reranking aspect of the pipeline, discussed in our previous research~\cite{DBLP:conf/paclic/SalnikovLRNBMP23-originalpaper}. Thus, with the question types listed above, we exclude {Yes/No} and {Count} questions. These question types offer no value information, as numbers and yes/no have no respective Wikidata entities, leading to a non-existent/impractical relationship between the question/answer pair. Thus, we deem {Yes/No} and {Count} pointless in the scope of our research. However, one could still utilize the entire pipeline to compute and evaluate the results based on the whole Mintaka dataset, including {Yes/No} and {Count}. 
% Referring to the question type classifier introduced in our original research ~\cite{DBLP:conf/paclic/SalnikovLRNBMP23-originalpaper}, this additional component allows for {Yes/No} and {Count} questions to receive special treatment.

We also compile and publish\footnote{\url{https://github.com/s-nlp/subgraph_kgqas-nlp/KGQA_Subgraphs_Ranking}} the dataset of subgraphs for the whole Mintaka dataset (for train, validation, and test splits separately). Additionally, we also publish the answer candidates generated by the base LMs. Subgraphs are collected using the process discussed in Section~\ref{sec:controllable_fusion:subgraph_extract}: we generate candidate answers, we take the true answer and the entities from the question entity neighbors as candidates, and construct subgraphs with Algorithm~\ref{alg:controllable_fusion:sub_extract}. As a result, we construct a ``correct'' subgraph containing the correct highlighted answer and several ``incorrect'' subgraphs from the incorrect candidate answers generated by the model. 
We present four versions of the dataset with subgraphs: with candidates generated by {T5-Large-SSM}, {T5-XL-SSM}, {Mistral}, and {Mixtral} models.

\subsection{Question Entities} \label{ques_ent}
Referencing the subgraph extraction protocol discussed in~\ref{sec:controllable_fusion:subgraph_extract}, we require question entities and the answer entity for each question-answer pair. Regarding the question entities, any entity linker such as {mGENRE}~\cite{decao2021multilingual} could be applied. However, with the objectives outlined above, utilizing an entity linker would derail the main focus of evaluating this reranking scope. As a result, we leverage the golden truth question entities provided by the Mintaka dataset. 

\subsection{Text Embeddings} \label{mpnet_explain}
Some features derived from the extracted subgraphs are in their natural language form (answer candidates for semantic ranker, Graph2Text sequences, and text features). Therefore, we use these features for our reranking objectives based on the MPNet embedding model~\cite{DBLP:conf/nips/Song0QLL20}. In Performance Sentence Embeddings (evaluation of the quality of the embedded sentence) and Performance Semantic Search (evaluation of the quality of the embedded search queries \& paragraph), the MPNet Embedding model outperforms 37 other sentence transformer models~\cite{reimers-2019-sentence-bert}. These models were compared by averaging the Performance Sentence Embeddings and Performance Semantic Search while considering the speed and the model size. In addition to the highest embedding performance, MPNet is relatively small while fast in training time. Moreover, this model is very popular within the HuggingFace community \footnote{\url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}}. Utilizing a well-known and widespread embedding model would enhance the aim of formulating our approach as a reranking problem motivated by end-user requirements. 

% TODO <---

\subsection{Graph2Text with Highlight \& Context} \label{hl_context}
As mentioned in~\ref{sec:controllable_fusion:g2t_seq}, we focus on different text representations of the subgraphs to rank the respective answer candidates. 
% According to our previous study, using \texttt{Graphomer}, the raw subgraph itself was ineffective as we could not encode the question to the subgraph \cite{DBLP:conf/paclic/SalnikovLRNBMP23-originalpaper}. Thus, with the linearised version of the subgraph byitself, Graphormer struggled to identify the subgraph with the correct answer candidate. 
We employ context for linearised text representation of the subgraph. This addition is a simple concatenation between the question and the linearised sequence, separated by a special token $<$/s$>$ to emphasize the question in the question-answer pairing. Moreover, the study showcased the effectiveness of highlighting~(HL) the answer candidate within the linearised sequence of the concatenation. 
The context is motivated by the assumption that the subgraph alone does not provide the necessary information to answer the question. In other words, the model cannot answer the question without it. Similarly, it is difficult to rank the answers if the model has no idea which entities in the subgraph are potential answer candidates.
% With this highlighting scheme, we increased the {Hits@1} by $2\%$, from $0.36 \rightarrow 0.38$ \cite{DBLP:conf/paclic/SalnikovLRNBMP23-originalpaper}. 
An example of such concatenation, the rank by MPNet to achieve the current state-of-the-art, is shown below:  

% \begin{blockquote}
\textbf{Question}: Which actor was the star of Titanic and was born in Los Angeles, California?

\textbf{Answer}: Leonardo DiCaprio
\vskip 0.15in
% \textbf{Original Deterministic Sequence with HL and Context\cite{DBLP:conf/paclic/SalnikovLRNBMP23-originalpaper}}:
\textbf{Sequence with HL and Context}:

Which actor was the star of Titanic and was born in Los Angeles, California? $<$/s$>$ [unused1]Leonardo DiCaprio[unused2], place of birth, Los Angeles, Titanic, cast member, [unused1]Leonardo DiCaprio[unused2]
% \end{blockquote}

\noindent We employ a similar \textbf{HL} and \textbf{context} protocol to our three Graph2Text sequences. The objective is to assist the model in understanding the question-answer pair and the Graph2Text sequence. An example of such processing for the three sequences can be seen below:

% \begin{blockquote}
\textbf{Question}: Which actor was the star of Titanic and was born in Los Angeles, California?

\textbf{Answer}: Leonardo DiCaprio
\vskip 0.15in
\textbf{Graph2Text Deterministic}: Which actor was the star of Titanic and was born in Los Angeles, California? $<$/s$>$ [unused1]Leonardo DiCaprio[unused2], place of birth, Los Angeles, Titanic, cast member, [unused1]Leonardo DiCaprio[unused2]
\vskip 0.1in
\textbf{Graph2Text T5}: Which actor was the star of Titanic and was born in Los Angeles, California?$<$/s$>$ Los Angeles born [unused1]Leonardo DiCaprio [unused2], who played the role of Jack Sparrow in the film Titanic, was born in the United States.
\vskip 0.1in
\textbf{Graph2Text GAP}: Which actor was the star of Titanic and was born in Los Angeles, California?$<$/s$>$Born in Los Angeles, the actor, [unused1]Leonardo DiCaprio [unused2], was a member of the crew of the Titanic.
% \end{blockquote}

\noindent It is important to note that we employ the \textbf{context} and \textbf{HL} approaches only for the MPNet approach, discussed in Section~\ref{sec:controllable_fusion:ranker}. Sensibly, the transformer-based model trained on sentences and paragraphs is adept at extracting useful information from natural texts. Thus, unlike our other proposed approaches, the MPNet approach classically only handles text embedding as input. Therefore, we utilize \textbf{context} and \textbf{HL} to give the model the best chance at extracting useful information to rank answer candidates. Other approaches, such as regression-based and gradient boosting, have various other features (i.e., graph, text, and sequence). Thus, we choose not to employ \textbf{context} and \textbf{HL} transformation for Graph2Text sequences for regression-based and gradient-boosting rankers. We discuss the pipeline for each ranker in more detail in Section~\ref{sec:controllable_fusion:experimental_pipeline}.


\subsection{Experimental Pipeline} \label{sec:controllable_fusion:experimental_pipeline}
With our two objectives of observing the effects of 1) different combinations of feature sets and 2) different reranking approaches in varying complexity, we devise our experiments for each answer candidate source (from either T5-Large-SSM, T5-XL-SSM, Mistral, or Mixtral)  as the following: 
\begin{itemize}
    \item Apply each feature set extracted from the subgraphs (from the least to most complex) to each proposed ranker (from the least to most complex). The feature sets $A, B, C$ are ranked from least to most complex. We feed $A$, then $B$, then $C$ to each ranker. The goal is to observe the performance of varying complexity feature sets with varying complexity rankers. 
    \item Add/Combine different feature sets (from the least to most complex) to each proposed ranker (from the least to most complex). The feature sets $A, B, C$ are ranked from least to most complex. We feed $A$, then $A+B$, then $A+B+C$ to each ranker. The goal is to observe how adding more complex feature sets affects the performance of each ranker. 
\end{itemize}
With the above experimental pipeline, we seek to provide an exhaustive case study on reranking LLM's answer candidates with rankers and feature sets of varying complexity. 


\subsection{Evaluation} \label{sec:controllable_fusion:evaluation}
As outlined in~\ref{sec:controllable_fusion:experimental_pipeline}, we experiment with numerous feature sets and rankers. Leveraging the limited amount of answer candidates, the primary objective is to determine whether selecting a final answer from this set is appropriate for our question-answering task. We evaluate using the {Hits@N} metric for all experiments discussed. Even if the top answer (Hits@1) is incorrect, this metric {Hits@N} allows us to understand the potential effectiveness of the respective ranker. For example, let us define two QA systems that can provide dozens of possible answers. These two systems generate the correct answer at the second and tenth positions. From an end-user point of view, the system that provides the correct answer at the second position or earlier in the sequence of answers will be much more beneficial. With that being said, as our research's main focus is the reranking task, Hits@N will provide us with valuable insights into each feature set and ranker.   
 
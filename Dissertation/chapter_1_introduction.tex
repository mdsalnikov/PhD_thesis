\chapter*{Introduction}
\label{chap:introduction}
\addcontentsline{toc}{chapter}{Introduction}

\textbf{Background and Relevance of the Work.}
For a long time, a key goal in artificial intelligence has been to make machines that can understand and answer human questions using large collections of structured knowledge. Knowledge Graphs (KGs) are good for this because they can show how facts are connected, which helps give exact answers. The arrival of Large Language Models (LLMs) has changed this area a lot. LLMs are trained on huge amounts of text and are very good at understanding and creating language. They also store a lot of factual knowledge inside themselves. However, when we use LLMs for Knowledge Graph Question Answering (KGQA), they have some problems. A big problem is that they can create answers that sound good but are wrong. This is often called hallucination~\cite{lin-etal-2022-truthfulqa, DBLP:conf/emnlp/RobertsRS20, DBLP:journals/csur/JiLFYSXIBMF23, DBLP:journals/corr/abs-2401-01313}. Also, it is hard to understand how LLMs come up with their answers, so it's difficult to check if the answers are based on facts.


Now that LLMs are used so much, KGs are even more important as a source of true, organized knowledge. LLMs are good at creating natural text and understanding general meanings. But they are not so good at handling facts that change often. They also have trouble adding new information without messing up the knowledge they already have. For example, LLMs alone often cannot answer comparative questions correctly if they do not have access to structured information, as shown in our work on the CAM 2.0 system~\cite{DBLP:conf/coling/ShalloufHSVMPBN24}. Our research in~\cite{pletenev-etal-2025-much} looked at the problems of adding new facts into LLMs using Low-rank Adaptation (LoRA). This study showed that using LoRA to add new knowledge can be harmful. The LLM might become worse at general question answering. This happens especially if the new knowledge is mostly about certain things. The LLM might then often give those overrepresented answers and be less likely to say when it's not sure. These results show that we really need good ways to combine the text-making skills of LLMs with the facts from KGs. We should not just try to update the LLM's own knowledge. This dissertation works on this problem. It offers and tests new methods for mixing LLMs and KGs better, to make KGQA systems more trustworthy.

\textit{Note: In some portion of this document (20-30\% of the entire text); Artificial Intelligence assistant, particularly Generative AI, has been used to improve, rephrase, shorten, or summarize the content. The technologies used include Gemini 2.5 Pro, Grok and Perplexity. All generated content was manually reviewed and edited to ensure accuracy and coherence.}

\textbf{Dissertation Objectives.}
This dissertation wants to create and test new ways to combine Large Language Models with Knowledge Graphs. The goal is to make Question Answering systems that are more accurate, dependable, and controllable. The main objectives are:
\begin{itemize}
    \item To make better answer candidates in KGQA. We can do this by using the LLMs' ability to understand meaning, together with specific type rules from KGs. This includes creating a method called Answer Candidate Type (ACT) Selection. This method predicts the likely semantic type of an answer. It then uses this type to filter and improve candidate answers (Chapter~\ref{chap:act_selection} has more details).
    \item To facilitate controllable and standardized community research on the fusion of Knowledge Graphs (KGs) and Large Language Models (LLMs) for KGQA, through the development and public release of the \texttt{ShortPathQA} dataset. This resource provides pre-computed KG subgraphs, allowing researchers to concentrate on fusion methodologies rather than complex KG processing tasks (as introduced in Chapter~\ref{sec:controllable_fusion:dataset}).
    \item To make LLM-generated answers more factually correct and controllable. We can do this by creating a system to re-rank candidate answers. This system uses structural proof from KG subgraphs. This includes finding important subgraphs that connect question entities to LLM-generated candidates. It also includes getting different features from these subgraphs and using various rankers to choose candidates that have stronger KG-based proof (Chapter~\ref{chap:controllable_fusion} has more details).
\end{itemize}

\textbf{Scientific Novelty.}
The new scientific ideas in this dissertation are in creating and testing new ways to mix LLMs and KGs. These ways help solve important problems in KGQA:
\begin{enumerate}
    \item \textbf{New Answer Candidate Typing and Selection:} The Answer Candidate Type~(ACT) Selection method (Chapter~\ref{chap:act_selection}, \cite{DBLP:journals/corr/abs-2310-07008}) is a new way to improve KGQA. It is special because it combines an LLM's skill to guess the semantic type of an answer with the clear type information from a KG (for example, Wikidata's P31 `instance of' property). This type-based checking of LLM-generated candidates works like a strong filter and scoring tool. It greatly improves the quality of candidate answers, even if the LLM first gives the wrong answer but gets its type right.
    \item \textbf{Controllable Fusion via Subgraph Reranking:} This work proposes a novel framework for improving the factuality of LLM outputs by reranking answer candidates based on structural evidence from KG subgraphs (Chapter~\ref{chap:controllable_fusion}, \cite{DBLP:journals/corr/abs-2310-02166}). The novelty lies in:
    \begin{itemize}
        \item The systematic extraction of relevant KG subgraphs connecting question entities to multiple LLM-generated answer candidates.
        \item The comprehensive feature engineering from these subgraphs, encompassing graph-theoretic metrics (e.g., PageRank, Katz centrality, density), textual features (question-answer concatenation), and various Graph2Text (G2T) sequence representations (Deterministic, T5-based, and GAP-based).
        \item The application and comparative analysis of diverse ranking models (from simple regression to neural rankers like MPNet) to effectively utilize these multi-modal subgraph features for promoting factually grounded answers.
    \end{itemize}
    \item \textbf{Creation of a Specialized Evaluation Resource (ShortPathQA):} The development and publication of the \texttt{ShortPathQA} dataset (Chapter~\ref{sec:controllable_fusion:dataset}, \cite{DBLP:conf/nldb/SalnikovSPQA25}) is a significant novel contribution. This dataset directly supports the research on Controllable Fusion via Subgraph Reranking presented in this dissertation and aims to foster further community exploration of such methods. As the first QA resource offering pre-computed KG subgraphs, \texttt{ShortPathQA} allows researchers to focus on the core tasks of subgraph reasoning and reranking. This isolates these tasks from the complexities of upstream entity linking and large-scale KG processing, thereby facilitating more targeted evaluation and development of controllable fusion techniques.
\end{enumerate}

\textbf{Structure and volume of the dissertation.}
This dissertation is structured to systematically present these contributions across four chapters, with a logical progression from foundational concepts through individual methodologies to integrated system implementations and system demonstrations. Introduction and Chapter~\ref{chap:related_work} provides a detailed overview of the background and relevance of the work. Chapter~\ref{chap:act_selection} introduces the Answer Candidate Type Selection method. Chapter~\ref{chap:controllable_fusion} introduces the Controllable Fusion via Subgraph Reranking method. Chapter~\ref{chap:system_demos} introduces the system demonstrations for the ACT Selection and tools for supporting Controllable Fusion methods.

\textbf{Theoretical and Practical Significance.}
The research presented in this dissertation holds both theoretical and practical significance for the fields of Natural Language Processing and Artificial Intelligence, particularly in the domain of Knowledge Graph Question Answering.

Theoretically, this work contributes to a deeper understanding of how the complementary strengths of LLMs (which are good at semantic understanding and fluency) and KGs (which provide structured factual knowledge and verifiability) can be synergistically combined. It explores specific mechanisms for this fusion, moving beyond simple retrieval augmentation towards more nuanced methods of candidate refinement and evidence-based reranking. The investigation into type-based filtering (ACT Selection) sheds light on the implicit semantic knowledge captured by LLMs and how it can be explicitly leveraged in conjunction with KG schema for improved reasoning. The study of subgraph-based reranking contributes to theories of evidence-based reasoning in QA, demonstrating how structural path information in KGs can serve as a robust signal for factuality and control in LLM outputs. Furthermore, the creation of the \texttt{ShortPathQA} dataset facilitates more focused theoretical explorations into subgraph reasoning by providing a standardized benchmark.

Practically, the methodologies developed, particularly ACT Selection and subgraph-based reranking, offer practical pathways to improve the factual accuracy and reliability of QA systems. This is crucial for real-world applications where incorrect information can have significant consequences. By making LLM outputs more controllable and grounded in verifiable KG facts, this research contributes to building more trustworthy AI systems. The ability to trace an answer back to KG evidence, as facilitated by subgraph analysis, enhances interpretability. The ACT Selection method offers a resource-efficient strategy to boost the performance of even smaller LLMs, making high-quality KGQA more accessible when computational resources are constrained. The \texttt{ShortPathQA} dataset provides a valuable, ready-to-use resource for other researchers and practitioners, lowering the barrier to entry for developing and evaluating LLM-KG fusion techniques without the need for extensive KG infrastructure setup. Finally, the system demonstrations (Chapter~\ref{chap:system_demos}), including API endpoints and visualization tools, provide tangible proofs-of-concept and reusable components for building advanced KGQA applications.

\textbf{Research Methodology.}
The research in this dissertation employs a multifaceted methodology combining theoretical development with empirical experimentation and system implementation.
\begin{enumerate}
    \item \textbf{Literature Review and Problem Formulation:} The work begins with a comprehensive review of existing research in KGQA, LLMs, and LLM-KG fusion strategies (Chapter~\ref{chap:related_work}) to identify gaps and formulate precise research questions.
    \item \textbf{Method Development - ACT Selection (Chapter~\ref{chap:act_selection}):}
    \begin{itemize}
        \item Observation of LLM behavior regarding answer type prediction, even in cases of factual error.
        \item Design of a pipeline that:
            \item Generates initial answer candidates using LLMs (e.g., T5, BART with Diverse Beam Search).
            \item Employs multilingual entity linking (e.g., mGENRE) to connect question and candidate entities to Wikidata.
            \item Infers the expected answer type using the LLM and aggregates type information from KG `instance-of` relations.
            \item Ranks candidates using a weighted scoring mechanism incorporating type scores, KG neighborhood scores, LLM generation scores, and question-property similarity.
    \end{itemize}
    \item \textbf{Method Development - Controllable Fusion via Subgraph Reranking (Chapter~\ref{chap:controllable_fusion}):}
    \begin{itemize}
        \item Hypothesis that KG path-based evidence can improve LLM answer factuality.
        \item Design of a reranking pipeline that:
            \item Generates multiple answer candidates from LLMs using Diverse Beam Search.
            \item Extracts KG subgraphs (shortest paths) connecting question entities to each candidate answer using Wikidata.
            \item Engineers diverse features from these subgraphs:
                \item Graph features: number of nodes/edges, cycles, bridges, average shortest path, density, Katz centrality, PageRank.
                \item Text features: concatenation of question and answer, encoded with MPNet.
                \item Graph2Text (G2T) features: using Deterministic linearization, T5-based G2T, and GAP-based G2T models, often with question context and answer highlighting.
            \item Employs and compares various reranking models: semantic (MPNet cosine similarity), regression (Linear, Logistic), gradient boosting (CatBoost), and neural (MPNet with regression head).
    \end{itemize}
    \item \textbf{Dataset Creation (ShortPathQA - Chapter~\ref{sec:controllable_fusion:dataset}):}
    \begin{itemize}
        \item Addressing the need for a focused benchmark for subgraph-based reasoning.
        \item Collection of questions from Mintaka (filtered for entity answers) and manual curation of new complex questions.
        \item Generation of answer candidates using LLMs.
        \item Unified shortest path subgraph extraction from Wikidata for all question-candidate pairs.
        \item Publication of the dataset with questions, candidates, and pre-computed subgraphs.
    \end{itemize}
    \item \textbf{Experimental Evaluation:}
    \begin{itemize}
        \item Utilization of established KGQA datasets (SQWD, RuBQ, Mintaka) and the newly created ShortPathQA.
        \item Fine-tuning of LLMs (T5, BART, spaCy NER models) on relevant training splits.
        \item Evaluation metrics appropriate for the tasks: Hits@1 for factoid accuracy in ACT Selection and ShortPathQA classification; Hits@N (N=1,2,3) and F1-score for reranking performance in controllable fusion and ShortPathQA.
        \item Ablation studies to assess the contribution of individual components and feature sets.
        \item Error analysis to understand model behavior and the corrective capabilities of the proposed methods.
        \item Comparison against strong baselines, including standalone LLMs (e.g., ChatGPT) and existing KGQA systems.
    \end{itemize}
    \item \textbf{System Implementation and Demonstration (Chapter~\ref{chap:system_demos}):}
    \begin{itemize}
        \item Development of practical system pipelines (baseline Seq2Seq, M3M, ACT Selection) using Python, FastAPI, PyTorch, HuggingFace Transformers, spaCy, etc.
        \item Creation of a subgraph visualization tool using web technologies (HTML, JavaScript, D3.js) to support research and provide interactive exploration.
    \end{itemize}
\end{enumerate}

\textbf{Validation of Research Results and Reliability.}
The research findings presented in this dissertation are validated through rigorous empirical evaluation on multiple standard benchmark datasets and newly created resources. The reliability of the results is ensured by:
\begin{itemize}
    \item \textbf{Standardized Datasets and Metrics:} For the ACT Selection method (Chapter~\ref{chap:act_selection}), evaluations were performed on three Wikidata-based one-hop KGQA datasets: SimpleQuestions-Wikidata (SQWD), RuBQ (English translations), and a subset of Mintaka (one-hop English questions). The primary metric was Hits@1, which directly measures the accuracy of the top-ranked answer.
    \item For the controllable fusion methods (Chapter~\ref{chap:controllable_fusion}), experiments were conducted on the Mintaka dataset (excluding count and yes/no questions) and the newly introduced ShortPathQA dataset. Evaluation metrics included Hits@N (N=1, 2, 3) to assess the reranker's ability to position the correct answer within the top N candidates, and F1-score for binary classification tasks on ShortPathQA.
    \item \textbf{Comparison with Baselines:} The performance of the proposed methods was consistently compared against various baselines.
        \item In ACT Selection, comparisons included the base Text-to-Text models (T5 and BART variants, both zero-shot and fine-tuned) without ACT, specialized KGQA systems like QAnswer and KEQA, and large conversational models like ChatGPT.
        \item In controllable fusion, baselines included initial LLM predictions (T5-Large-SSM, T5-XL-SSM, Mistral, Mixtral), random ranking, semantic reranking, and simpler regression-based models before evaluating more complex neural rankers. For ShortPathQA, comparisons included zero-shot LLMs (GPT-4o, LLaMA3-8b-Instruct) and supervised models (MPNet, fine-tuned LLaMA3-8b-Instruct).
    \item \textbf{Ablation Studies:} Comprehensive ablation studies were conducted (e.g., Table~\ref{tab:act_selection:ablation_study} for ACT Selection, and analyses in Chapter~\ref{chap:controllable_fusion} for feature set combinations) to assess the individual contributions of different components of the proposed frameworks (e.g., different scoring mechanisms, feature types). This helps to isolate the impact of novel elements.
    \item \textbf{Error Analysis:} Qualitative error analysis was performed (e.g., in Chapter~\ref{chap:act_selection} and for G2T methods in Chapter~\ref{chap:controllable_fusion}) to understand specific instances where the proposed methods succeed or fail, providing deeper insights beyond aggregate metrics. For example, ACT Selection demonstrated an ability to predict correct answer types in 94\% of instances for T5-Large-SSM on SQWD, a significant improvement over the 61\% baseline. The G2T analysis highlighted T5's superior factual accuracy over GAP in preserving entity information.
    \item \textbf{Cross-Model and Cross-Dataset Evaluation:} The proposed methods were evaluated across different LLM architectures (T5, BART, Mistral, Mixtral) and sizes, and across multiple datasets with varying characteristics, demonstrating the robustness and generalizability of the findings. For example, ACT Selection consistently improved performance across all tested base models and datasets. The subgraph reranking methods also showed consistent Hits@1 improvements across different LLMs.
    \item \textbf{Statistical Significance and Reproducibility:} While not always explicitly stated as statistical significance tests, the consistent and considerable margins of improvement over baselines across multiple setups suggest meaningful results. The publication of code (e.g., M3M pipeline, ShortPathQA dataset and code) and detailed experimental setups (hyperparameters, model versions) supports reproducibility. For the \texttt{ShortPathQA} dataset, a manual test set was curated with multiple annotators, achieving a Cohen's kappa of 0.81 for question type and an average quality score of 4.81/5, ensuring data quality.
\end{itemize}
These measures collectively ensure that the conclusions drawn are well-supported by empirical evidence and that the proposed methods offer reliable improvements in KGQA.

\textbf{Approbation of the Work and Publications.}
The research presented in this dissertation has been disseminated through peer-reviewed publications in international conferences and journals, and through participation in shared tasks and system demonstrations. The key contributions have been presented to and validated by the scientific community.

The publications by the author that form the basis of or are related to this dissertation are:
\begin{enumerate}
    \item \textbf{[Scopus, Conference Proceedings, CORE A*]} Anton Razzhigaev*, \textbf{Mikhail Salnikov*}, Valentin Malykh, Pavel Braslavski, and Alexander Panchenko. 2023. A System for Answering Simple Questions in Multiple Languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 524–537, Toronto, Canada. Association for Computational Linguistics.
    \item \textbf{[Conference Proceedings]} \textbf{Mikhail Salnikov}, Maria Lysyuk, Pavel Braslavski, Anton Razzhigaev, Valentin A. Malykh, and Alexander Panchenko. 2023. Answer Candidate Type Selection: Text-To-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs. In Proceedings of the 19th Conference on Natural Language Processing (KONVENS 2023), pages 155–164, Ingolstadt, Germany. Association for Computational Linguistics. 
    \item \textbf{[Scopus, Conference Proceedings, CORE C]} \textbf{Mikhail Salnikov}, Hai Le, Prateek Rajput, Irina Nikishina, Pavel Braslavski, Valentin Malykh, and Alexander Panchenko. 2023. Large Language Models Meet Knowledge Graphs to Answer Factoid Questions. In Proceedings of the 37th Pacific Asia Conference on Language, Information and Computation, pages 635–644, Hong Kong, China. Association for Computational Linguistics.
    \item \textbf{[Conference Proceedings]} Andrey Sakhovskiy*, \textbf{Mikhail Salnikov*}, Irina Nikishina, Aida Usmanova, Angelie Kraft, Cedric Möller, Debayan Banerjee, Junbo Huang, Longquan Jiang, Rana Abdullah, Xi Yan, Dmitry Ustalov, Elena Tutubalina, Ricardo Usbeck, and Alexander Panchenko. 2024. TextGraphs 2024 Shared Task on Text-Graph Representations for Knowledge Graph Question Answering. In Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing, pages 116–125, Bangkok, Thailand. Association for Computational Linguistics.
    \item \textbf{[Scopus]} \textbf{Mikhail Salnikov*}, Andrey Sakhovskiy*, Irina Nikishina, Aida Usmanova, Angelie Kraft, Cedric Möller, Debayan Banerjee, Junbo Huang, Longquan Jiang, Rana Abdullah, Xi Yan, Elena Tutubalina, Ricardo Usbeck, and Alexander Panchenko. (2025). ShortPathQA: A Dataset for Controllable Fusion of Large Language Models with Knowledge Graphs. In \textit{Natural Language Processing and Information Systems - 30th International Conference on Applications of Natural Language to Information Systems, NLDB 2025}.
    \item \textbf{[Scopus, Conference Proceedings, CORE B]} Ahmad Shallouf*, Hanna Herasimchyk*, \textbf{Mikhail Salnikov*}, Rudy Alexandro Garrido Veliz*, Natia Mestvirishvili*, Alexander Panchenko, Chris Biemann, and Irina Nikishina. 2024. CAM 2.0: End-to-End Open Domain Comparative Question Answering System. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 2657–2672, Torino, Italia. ELRA and ICCL.
    \item \textbf{[Scopus, WoS]} Maria Lysyuk, \textbf{Mikhail Salnikov}, Pavel Braslavski, and Alexander Panchenko. (2024). Konstruktor: A Strong Baseline for Simple Knowledge Graph Question Answering In International Conference on Applications of Natural Language to Information Systems, pp. 107-118. Cham: Springer Nature Switzerland, 2024.
    \item \textbf{[Scopus, Conference Proceedings]} Dmitrii Iarosh, Alexander Panchenko, and \textbf{Mikhail Salnikov}. (2025). On Reducing Factual Hallucinations in Graph-to-Text Generation Using Large Language Models. In \textit{Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)}.
    \item \textbf{[Scopus, Conference Proceedings, CORE A]} Sergey Pletenev, Maria Marina, Daniil Moskovskiy, Vasily Konovalov, Pavel Braslavski, Alexander Panchenko, and \textbf{Mikhail Salnikov}. 2025. How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 4309–4322, Albuquerque, New Mexico. Association for Computational Linguistics.
\end{enumerate}

\textbf{Author's Personal Contribution.}

Author's personal contribution to the research presented in this dissertation is as follows:
\begin{itemize}
    \item \textbf{ACT Selection Method Development (Chapter~\ref{chap:act_selection}):} The author was fully responsible for the conceptualization, implementation, and experimental validation of the Answer Candidate Type Selection pipeline, including the design of the four-component scoring mechanism.
    \item \textbf{Subgraph Reranking Framework (Chapter~\ref{chap:controllable_fusion}):} The author led the full development of the controllable fusion methodology and the design of the experiments. The implementation and experimental design for the Graph-To-Text (G2T) models were developed by the author's advisor with significant contributions from Hai Le, Dmitrii Iarosh, and Ivan Lazichny. Olga Tsymboi and Egor Cheremiskin implemented experiments with the Mistral and Mixtral models.
    \item \textbf{ShortPathQA Dataset Creation (Chapter~\ref{sec:controllable_fusion:dataset}):} The author had full responsibility for the dataset design and the subgraph generation pipeline. The annotation framework and question curation for the manual portion of the dataset were handled by Andrey Sakhovskiy and Irina Nikishina. Andrey Sakhovskiy conducted experiments with encoder-only baselines.
    \item \textbf{System Demonstration Creation (Chapter~\ref{chap:system_demos}):} The author had full responsibility for creating the system demonstrations. Dmitrii Iarosh assisted with the development of the subgraph visualization tool and related DevOps tasks.
\end{itemize}


\chapter{Conclusion and Future Work}
\label{chap:conclusion}

% 7.1 Synthesis of Contributions
\section{Synthesis of Contributions}
\label{sec:conclusion_synthesis}
% Comment: Provide a comprehensive summary of the thesis's achievements. Reiterate the 
% novel methods for KG-LLM fusion, the ShortPathQA benchmark contribution, system 
% demonstrations, and insights from related studies (LoRA). Emphasize the overall 
% narrative: identifying the LLM factuality problem and providing KG-based solutions.


% 7.2 Revisiting Research Questions
\section{Revisiting Research Questions}
\label{sec:conclusion_revisiting_rqs}
% Comment: Directly address the research questions posed in Chapter 1, providing 
% concise answers based on the evidence presented throughout the thesis.


% 7.3 Limitations of the Presented Work
\section{Limitations of the Presented Work}
\label{sec:conclusion_limitations}
% Comment: Honestly discuss the limitations of your research. This might include the 
% scope (e.g., focus on factoid QA), the specific KGs or LLMs used, scalability 
% challenges of the proposed methods, or aspects not fully explored.


% 7.4 Future Research Directions
\section{Future Research Directions}
\label{sec:conclusion_future_work}
% Comment: Propose concrete directions for future work building upon your thesis. 
% Examples: exploring more complex reasoning tasks, applying methods to different 
% domains or languages, integrating dynamic/temporal KGs, improving the efficiency 
% and scalability of fusion methods, developing more sophisticated control mechanisms, 
% investigating user interaction with KG-aware LLMs.

\chapter{Exploring the Limits of LoRA for LLM-KG Integration}
\label{chap:lora_limits}

\section{Introduction}
\label{sec:lora:intro}
This chapter explores an alternative approach to integrating Large Language Models (LLMs) with Knowledge Graphs (KGs): parameter-efficient fine-tuning through Low-Rank Adaptation (LoRA). While previous chapters focused on methods that keep the LLM and KG as separate components, with various fusion strategies at inference time, this chapter investigates the potential of directly adapting the LLM's parameters to better incorporate factual knowledge from KGs. We specifically examine the capabilities and limitations of LoRA in this context, providing insights into when this approach might be preferable to the external fusion methods explored earlier.

\section{Background on Parameter-Efficient Fine-Tuning}
\label{sec:lora:background}
% TODO: Provide background on PEFT and LoRA in particular.
% TODO: Explain how LoRA works and its advantages for adaptation.

\section{Experimental Setup}
\label{sec:lora:setup}
% TODO: Describe the experimental design for evaluating LoRA.
% TODO: Detail the models, datasets, and metrics used.

\section{Results and Analysis}
\label{sec:lora:results}
% TODO: Present the experimental results.
% TODO: Analyze where LoRA succeeds and where it falls short.

\section{Comparative Evaluation with Fusion Methods}
\label{sec:lora:comparison}
% TODO: Compare LoRA adaptation with the fusion methods from earlier chapters.
% TODO: Discuss trade-offs in terms of performance, efficiency, and deployment.

\section{Discussion on the Limits of LoRA for Knowledge Integration}
\label{sec:lora:discussion}
% TODO: Discuss the fundamental limitations of parameter-efficient fine-tuning for factual knowledge.
% TODO: Address potential future directions and improvements.

\section{Chapter Summary}
\label{sec:lora:summary}
% TODO: Summarize the key findings regarding LoRA's capabilities and limitations. 